{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-to-User (Collaborative Filtering algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the missing value in the movie_rating.xlsx dataset, we use collaborative filtering method to build the algorithm. Firstly, we define the function,colaborative_filtering, to do the prediction work. After taking the whole movie_rating matrix and the number of neighbors,k, the function will return a data frame containing all the prediction value form the original matrix.\n",
    "\n",
    "Since the k is a parameter to be tuned, we then develop the cross_validation function to help us calculate the RMSE related to each k. Inside the cross_validation function, we also use function pred and rmse_df. The former function is constructed to give the actual prediction value given a list of target user_id and movie_id, and the latter function is built to give the RMSE, comparing the actual value and the prediction value.\n",
    "\n",
    "After having the RMSE dictionary, we use the k_optimal function to return the k related to the lowest RMSE in the testing set.\n",
    "\n",
    "With all the function developed, we first split the whole movie_rating dataset into a training set and a test set, with the ratio of 0.8 and 0.2. And then use cross_validation to find the RMSE related to k, whose range is between 10 and 19. Since the RMSE related to 19 is the smallest in the test set, we finally use colaborative_filtering and k = 19 to predict the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the data as a dataframe, observations (rows) are users, and attributes (columns) are items\n",
    "xls = pd.read_excel('movie_ratings_inclass.xlsx', sheetname = 'Sheet1' )\n",
    "df = xls.set_index('User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dedine the colaboration filtering method, which only choose the k nearest neighbour( sorted by correlation) to do the prediction\n",
    "def colaborative_filtering(df,k):# input is the matrix to be filled and k\n",
    "    cor = df.T.corr()\n",
    "    cor = cor.fillna(0) #fill the nan cell of correlation table with 0\n",
    "    mean_user = df.mean(axis = 1).tolist() #mean before fill the original matrix nan with 0)\n",
    "    new_df = df.fillna(0)#rebuild the matrix\n",
    "    for i in range(df.shape[0]):#user\n",
    "        for j in range(df.shape[1]):#item\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            neighbour = (np.argsort(cor).iloc[i,cor.shape[1]-k-1:cor.shape[1]-1]).tolist() #choose the neighbour, rank by correlation\n",
    "            for n in neighbour:#neignbour\n",
    "                if i!=n and (not pd.isnull(df.iloc[n,j])):#only count weight when the neighbour has rate the item\n",
    "                    denominator = denominator + abs(cor.iloc[i,n]) \n",
    "                    numerator = numerator + cor.iloc[i,n]*(df.iloc[n,j]-mean_user[n])\n",
    "            if denominator ==0:\n",
    "                new_df.iloc[i,j] =float(mean_user[i])\n",
    "            else:  \n",
    "                new_df.iloc[i,j] =float(mean_user[i]) + (float(numerator)/denominator)\n",
    "    return new_df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the value we want in the whole new matrix to be the new pediction\n",
    "def pred(df_pred,df_test):\n",
    "    prediction = []\n",
    "    for i in range(df_test.shape[0]):\n",
    "        pred = df_pred.loc[df_test.iloc[i,0],df_test.iloc[i,1]]\n",
    "        prediction.append(pred)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare the value in the give matrix and the value we predict\n",
    "def rmse_df(df, df_pred):\n",
    "    err = df.sub(df_pred)\n",
    "    n = np.sum(1-np.isnan(df)).sum()\n",
    "    se = np.power(err,2).sum().sum()\n",
    "    rmse = np.power(se/n,0.5)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use training set and test set to find the optimal number of neighbours, k , to be put in the colaboration filtering method\n",
    "def cross_validation(df_train, df_test,k_range):\n",
    "    rmse_dic = {}\n",
    "    for k in k_range:\n",
    "        df_pred= colaborative_filtering(df_train,k)\n",
    "        prediction = pd.DataFrame({'rating':pred(df_pred,df_test)})\n",
    "        rmse_dic[k] = rmse_df(df_test[['rating']],prediction)\n",
    "    return rmse_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#return the optimal k related to the lowest rmse \n",
    "def k_optimal(rmse_dic):\n",
    "    return min(rmse_dic, key = rmse_dic.get)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the whole dataset to trainig set and test set\n",
    "melt = pd.melt(xls, id_vars='User', \n",
    "               value_vars=list(df.columns[0:]),\n",
    "               var_name='movie_id', \n",
    "               value_name='rating')\n",
    "\n",
    "np.random.seed(1)  \n",
    "train2,test2 = train_test_split(melt,test_size = 0.2)\n",
    "df_train = train2.pivot(index = 'User', columns = 'movie_id', values = 'rating')\n",
    "df_test = test2.reset_index()[['User','movie_id','rating']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do cross validation to find optimal k\n",
    "k_range = range(10,df.shape[1])          \n",
    "rmse_dictionary =cross_validation(df_train,df_test,k_range)\n",
    "k_op = k_optimal(rmse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 1.7236967819610547,\n",
       " 11: 1.7128829348636185,\n",
       " 12: 1.7119369348474973,\n",
       " 13: 1.6754964360996423,\n",
       " 14: 1.6459935932464638,\n",
       " 15: 1.6499851195518964,\n",
       " 16: 1.6622488339525201,\n",
       " 17: 1.695511802858092,\n",
       " 18: 1.665066818326314,\n",
       " 19: 1.635576003194916}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_dictionary  # the optimal k is 19, with the rmse being the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.125887</td>\n",
       "      <td>3.809875</td>\n",
       "      <td>2.468465</td>\n",
       "      <td>2.997126</td>\n",
       "      <td>3.664587</td>\n",
       "      <td>3.102324</td>\n",
       "      <td>2.606409</td>\n",
       "      <td>2.673367</td>\n",
       "      <td>2.317369</td>\n",
       "      <td>3.521447</td>\n",
       "      <td>3.044063</td>\n",
       "      <td>2.934637</td>\n",
       "      <td>4.040158</td>\n",
       "      <td>2.998709</td>\n",
       "      <td>2.573670</td>\n",
       "      <td>4.267142</td>\n",
       "      <td>1.761662</td>\n",
       "      <td>4.332319</td>\n",
       "      <td>3.593288</td>\n",
       "      <td>2.319217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.988588</td>\n",
       "      <td>2.915932</td>\n",
       "      <td>3.115639</td>\n",
       "      <td>2.133721</td>\n",
       "      <td>3.057883</td>\n",
       "      <td>2.432871</td>\n",
       "      <td>2.279495</td>\n",
       "      <td>2.580919</td>\n",
       "      <td>3.000151</td>\n",
       "      <td>3.632670</td>\n",
       "      <td>3.096949</td>\n",
       "      <td>2.527106</td>\n",
       "      <td>2.173434</td>\n",
       "      <td>2.532439</td>\n",
       "      <td>2.473829</td>\n",
       "      <td>3.200056</td>\n",
       "      <td>3.763495</td>\n",
       "      <td>1.881988</td>\n",
       "      <td>2.689880</td>\n",
       "      <td>1.752868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.912954</td>\n",
       "      <td>2.094736</td>\n",
       "      <td>2.369515</td>\n",
       "      <td>3.683575</td>\n",
       "      <td>1.987374</td>\n",
       "      <td>2.466174</td>\n",
       "      <td>3.281397</td>\n",
       "      <td>1.978337</td>\n",
       "      <td>3.130212</td>\n",
       "      <td>1.539215</td>\n",
       "      <td>3.272523</td>\n",
       "      <td>2.279263</td>\n",
       "      <td>2.603694</td>\n",
       "      <td>2.982542</td>\n",
       "      <td>2.186740</td>\n",
       "      <td>1.321762</td>\n",
       "      <td>2.126969</td>\n",
       "      <td>2.364964</td>\n",
       "      <td>1.988240</td>\n",
       "      <td>3.365319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.326940</td>\n",
       "      <td>2.339844</td>\n",
       "      <td>2.388143</td>\n",
       "      <td>2.700540</td>\n",
       "      <td>2.575861</td>\n",
       "      <td>1.627711</td>\n",
       "      <td>2.875391</td>\n",
       "      <td>2.793358</td>\n",
       "      <td>3.117819</td>\n",
       "      <td>2.315239</td>\n",
       "      <td>2.864993</td>\n",
       "      <td>3.636110</td>\n",
       "      <td>1.807446</td>\n",
       "      <td>2.924626</td>\n",
       "      <td>3.392369</td>\n",
       "      <td>2.389743</td>\n",
       "      <td>3.752023</td>\n",
       "      <td>1.487925</td>\n",
       "      <td>2.007544</td>\n",
       "      <td>3.683318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.578885</td>\n",
       "      <td>2.501400</td>\n",
       "      <td>3.047309</td>\n",
       "      <td>2.395425</td>\n",
       "      <td>3.392122</td>\n",
       "      <td>2.622037</td>\n",
       "      <td>3.087190</td>\n",
       "      <td>3.083817</td>\n",
       "      <td>3.723310</td>\n",
       "      <td>2.617868</td>\n",
       "      <td>3.964756</td>\n",
       "      <td>3.011088</td>\n",
       "      <td>3.098395</td>\n",
       "      <td>3.409873</td>\n",
       "      <td>3.955804</td>\n",
       "      <td>2.317867</td>\n",
       "      <td>3.497197</td>\n",
       "      <td>3.571934</td>\n",
       "      <td>3.677112</td>\n",
       "      <td>3.248757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.901324</td>\n",
       "      <td>4.029951</td>\n",
       "      <td>3.009891</td>\n",
       "      <td>3.663975</td>\n",
       "      <td>3.521451</td>\n",
       "      <td>3.323299</td>\n",
       "      <td>3.007930</td>\n",
       "      <td>2.218895</td>\n",
       "      <td>2.993365</td>\n",
       "      <td>4.031558</td>\n",
       "      <td>3.873461</td>\n",
       "      <td>2.121943</td>\n",
       "      <td>4.000037</td>\n",
       "      <td>2.838593</td>\n",
       "      <td>1.655562</td>\n",
       "      <td>3.619244</td>\n",
       "      <td>2.748993</td>\n",
       "      <td>3.299288</td>\n",
       "      <td>2.454697</td>\n",
       "      <td>2.453458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.567399</td>\n",
       "      <td>1.529158</td>\n",
       "      <td>3.113060</td>\n",
       "      <td>2.175780</td>\n",
       "      <td>2.513765</td>\n",
       "      <td>3.185910</td>\n",
       "      <td>2.801474</td>\n",
       "      <td>3.567647</td>\n",
       "      <td>2.323389</td>\n",
       "      <td>2.179307</td>\n",
       "      <td>1.930943</td>\n",
       "      <td>2.658001</td>\n",
       "      <td>2.259856</td>\n",
       "      <td>2.175000</td>\n",
       "      <td>2.778515</td>\n",
       "      <td>1.777193</td>\n",
       "      <td>2.325736</td>\n",
       "      <td>2.792869</td>\n",
       "      <td>3.540116</td>\n",
       "      <td>2.824637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.514328</td>\n",
       "      <td>3.031754</td>\n",
       "      <td>1.988014</td>\n",
       "      <td>1.799454</td>\n",
       "      <td>2.775932</td>\n",
       "      <td>2.795472</td>\n",
       "      <td>2.251844</td>\n",
       "      <td>3.030631</td>\n",
       "      <td>2.282562</td>\n",
       "      <td>2.746412</td>\n",
       "      <td>2.828054</td>\n",
       "      <td>2.144753</td>\n",
       "      <td>2.837773</td>\n",
       "      <td>2.326532</td>\n",
       "      <td>3.466282</td>\n",
       "      <td>3.619402</td>\n",
       "      <td>1.919753</td>\n",
       "      <td>3.713291</td>\n",
       "      <td>4.156083</td>\n",
       "      <td>2.860030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.650760</td>\n",
       "      <td>1.960428</td>\n",
       "      <td>3.351168</td>\n",
       "      <td>3.515065</td>\n",
       "      <td>2.621152</td>\n",
       "      <td>3.134144</td>\n",
       "      <td>3.795093</td>\n",
       "      <td>3.731547</td>\n",
       "      <td>3.544526</td>\n",
       "      <td>2.412741</td>\n",
       "      <td>2.995182</td>\n",
       "      <td>3.572298</td>\n",
       "      <td>2.399526</td>\n",
       "      <td>3.780511</td>\n",
       "      <td>4.530988</td>\n",
       "      <td>1.999423</td>\n",
       "      <td>3.485569</td>\n",
       "      <td>2.459768</td>\n",
       "      <td>3.110284</td>\n",
       "      <td>4.022902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.895273</td>\n",
       "      <td>2.232604</td>\n",
       "      <td>3.870131</td>\n",
       "      <td>4.047578</td>\n",
       "      <td>2.657119</td>\n",
       "      <td>3.249615</td>\n",
       "      <td>3.713518</td>\n",
       "      <td>3.436299</td>\n",
       "      <td>4.137508</td>\n",
       "      <td>3.085499</td>\n",
       "      <td>3.176070</td>\n",
       "      <td>3.475341</td>\n",
       "      <td>2.360364</td>\n",
       "      <td>3.587308</td>\n",
       "      <td>3.287793</td>\n",
       "      <td>2.135742</td>\n",
       "      <td>4.102399</td>\n",
       "      <td>1.827758</td>\n",
       "      <td>2.260609</td>\n",
       "      <td>3.852409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.022385</td>\n",
       "      <td>1.583050</td>\n",
       "      <td>3.538895</td>\n",
       "      <td>3.738522</td>\n",
       "      <td>2.322645</td>\n",
       "      <td>3.539361</td>\n",
       "      <td>3.623261</td>\n",
       "      <td>3.105597</td>\n",
       "      <td>3.410328</td>\n",
       "      <td>2.394786</td>\n",
       "      <td>2.702986</td>\n",
       "      <td>3.240715</td>\n",
       "      <td>2.435015</td>\n",
       "      <td>3.194068</td>\n",
       "      <td>2.664088</td>\n",
       "      <td>1.636937</td>\n",
       "      <td>3.299081</td>\n",
       "      <td>2.259429</td>\n",
       "      <td>2.050339</td>\n",
       "      <td>3.394597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.472253</td>\n",
       "      <td>3.947486</td>\n",
       "      <td>2.047351</td>\n",
       "      <td>2.635083</td>\n",
       "      <td>3.061412</td>\n",
       "      <td>2.452677</td>\n",
       "      <td>2.013566</td>\n",
       "      <td>2.087771</td>\n",
       "      <td>2.568666</td>\n",
       "      <td>4.028625</td>\n",
       "      <td>2.512130</td>\n",
       "      <td>2.773786</td>\n",
       "      <td>3.304721</td>\n",
       "      <td>2.033686</td>\n",
       "      <td>2.196265</td>\n",
       "      <td>3.994017</td>\n",
       "      <td>1.667015</td>\n",
       "      <td>2.966209</td>\n",
       "      <td>3.031615</td>\n",
       "      <td>1.985445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.823899</td>\n",
       "      <td>3.543119</td>\n",
       "      <td>4.444036</td>\n",
       "      <td>4.026062</td>\n",
       "      <td>3.793119</td>\n",
       "      <td>4.394619</td>\n",
       "      <td>3.925320</td>\n",
       "      <td>4.226938</td>\n",
       "      <td>3.333401</td>\n",
       "      <td>3.939242</td>\n",
       "      <td>2.680818</td>\n",
       "      <td>3.029840</td>\n",
       "      <td>4.473829</td>\n",
       "      <td>3.975737</td>\n",
       "      <td>2.835887</td>\n",
       "      <td>3.519773</td>\n",
       "      <td>3.207538</td>\n",
       "      <td>4.253426</td>\n",
       "      <td>4.318568</td>\n",
       "      <td>3.229943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.014222</td>\n",
       "      <td>3.566829</td>\n",
       "      <td>2.107019</td>\n",
       "      <td>2.786270</td>\n",
       "      <td>2.705683</td>\n",
       "      <td>2.694189</td>\n",
       "      <td>2.081434</td>\n",
       "      <td>1.992951</td>\n",
       "      <td>2.431743</td>\n",
       "      <td>3.761554</td>\n",
       "      <td>2.868402</td>\n",
       "      <td>1.980609</td>\n",
       "      <td>3.012868</td>\n",
       "      <td>1.944916</td>\n",
       "      <td>2.147991</td>\n",
       "      <td>3.834179</td>\n",
       "      <td>1.541710</td>\n",
       "      <td>2.795476</td>\n",
       "      <td>3.171459</td>\n",
       "      <td>2.230990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.164221</td>\n",
       "      <td>3.502096</td>\n",
       "      <td>3.886642</td>\n",
       "      <td>2.358353</td>\n",
       "      <td>3.742648</td>\n",
       "      <td>3.733895</td>\n",
       "      <td>2.505191</td>\n",
       "      <td>2.661823</td>\n",
       "      <td>2.576640</td>\n",
       "      <td>4.332797</td>\n",
       "      <td>3.095291</td>\n",
       "      <td>2.249090</td>\n",
       "      <td>3.756429</td>\n",
       "      <td>2.332030</td>\n",
       "      <td>1.929799</td>\n",
       "      <td>4.202546</td>\n",
       "      <td>3.163350</td>\n",
       "      <td>3.466870</td>\n",
       "      <td>3.939457</td>\n",
       "      <td>2.038742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.006902</td>\n",
       "      <td>2.924975</td>\n",
       "      <td>2.117867</td>\n",
       "      <td>2.779568</td>\n",
       "      <td>2.481834</td>\n",
       "      <td>1.812072</td>\n",
       "      <td>3.007122</td>\n",
       "      <td>2.486361</td>\n",
       "      <td>3.197955</td>\n",
       "      <td>1.341647</td>\n",
       "      <td>3.016596</td>\n",
       "      <td>3.139452</td>\n",
       "      <td>3.066937</td>\n",
       "      <td>3.228790</td>\n",
       "      <td>3.727807</td>\n",
       "      <td>2.755635</td>\n",
       "      <td>2.270164</td>\n",
       "      <td>3.699232</td>\n",
       "      <td>2.519407</td>\n",
       "      <td>3.912726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.507914</td>\n",
       "      <td>3.341891</td>\n",
       "      <td>2.683169</td>\n",
       "      <td>2.300038</td>\n",
       "      <td>3.562616</td>\n",
       "      <td>2.949834</td>\n",
       "      <td>1.706754</td>\n",
       "      <td>2.656664</td>\n",
       "      <td>2.031370</td>\n",
       "      <td>3.960236</td>\n",
       "      <td>2.090184</td>\n",
       "      <td>2.912253</td>\n",
       "      <td>2.940406</td>\n",
       "      <td>2.399452</td>\n",
       "      <td>2.610773</td>\n",
       "      <td>3.999623</td>\n",
       "      <td>2.714805</td>\n",
       "      <td>3.082639</td>\n",
       "      <td>3.289360</td>\n",
       "      <td>1.601918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.294214</td>\n",
       "      <td>3.238111</td>\n",
       "      <td>2.600474</td>\n",
       "      <td>4.289855</td>\n",
       "      <td>2.391573</td>\n",
       "      <td>2.059570</td>\n",
       "      <td>3.632777</td>\n",
       "      <td>2.878757</td>\n",
       "      <td>3.496710</td>\n",
       "      <td>2.479210</td>\n",
       "      <td>3.745025</td>\n",
       "      <td>3.691556</td>\n",
       "      <td>2.841375</td>\n",
       "      <td>3.130646</td>\n",
       "      <td>3.487406</td>\n",
       "      <td>2.861135</td>\n",
       "      <td>3.137600</td>\n",
       "      <td>2.415914</td>\n",
       "      <td>2.365042</td>\n",
       "      <td>3.712216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.468367</td>\n",
       "      <td>1.942937</td>\n",
       "      <td>3.467535</td>\n",
       "      <td>2.315975</td>\n",
       "      <td>2.575120</td>\n",
       "      <td>2.936734</td>\n",
       "      <td>2.616287</td>\n",
       "      <td>2.204342</td>\n",
       "      <td>2.712550</td>\n",
       "      <td>3.073534</td>\n",
       "      <td>3.288228</td>\n",
       "      <td>1.735952</td>\n",
       "      <td>2.080328</td>\n",
       "      <td>1.840467</td>\n",
       "      <td>2.198956</td>\n",
       "      <td>1.675534</td>\n",
       "      <td>2.561586</td>\n",
       "      <td>2.003668</td>\n",
       "      <td>2.839879</td>\n",
       "      <td>2.348955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.131359</td>\n",
       "      <td>3.297008</td>\n",
       "      <td>2.393641</td>\n",
       "      <td>2.854704</td>\n",
       "      <td>3.558171</td>\n",
       "      <td>2.888006</td>\n",
       "      <td>2.835794</td>\n",
       "      <td>3.585306</td>\n",
       "      <td>2.282475</td>\n",
       "      <td>2.064412</td>\n",
       "      <td>2.515358</td>\n",
       "      <td>3.533807</td>\n",
       "      <td>3.549958</td>\n",
       "      <td>2.647616</td>\n",
       "      <td>3.894996</td>\n",
       "      <td>3.681273</td>\n",
       "      <td>2.450744</td>\n",
       "      <td>4.433131</td>\n",
       "      <td>3.504492</td>\n",
       "      <td>3.270193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             A         B         C         D         E         F         G  \\\n",
       "User                                                                         \n",
       "1     2.125887  3.809875  2.468465  2.997126  3.664587  3.102324  2.606409   \n",
       "2     3.988588  2.915932  3.115639  2.133721  3.057883  2.432871  2.279495   \n",
       "3     1.912954  2.094736  2.369515  3.683575  1.987374  2.466174  3.281397   \n",
       "4     3.326940  2.339844  2.388143  2.700540  2.575861  1.627711  2.875391   \n",
       "5     3.578885  2.501400  3.047309  2.395425  3.392122  2.622037  3.087190   \n",
       "6     2.901324  4.029951  3.009891  3.663975  3.521451  3.323299  3.007930   \n",
       "7     2.567399  1.529158  3.113060  2.175780  2.513765  3.185910  2.801474   \n",
       "8     3.514328  3.031754  1.988014  1.799454  2.775932  2.795472  2.251844   \n",
       "9     3.650760  1.960428  3.351168  3.515065  2.621152  3.134144  3.795093   \n",
       "10    3.895273  2.232604  3.870131  4.047578  2.657119  3.249615  3.713518   \n",
       "11    3.022385  1.583050  3.538895  3.738522  2.322645  3.539361  3.623261   \n",
       "12    2.472253  3.947486  2.047351  2.635083  3.061412  2.452677  2.013566   \n",
       "13    2.823899  3.543119  4.444036  4.026062  3.793119  4.394619  3.925320   \n",
       "14    3.014222  3.566829  2.107019  2.786270  2.705683  2.694189  2.081434   \n",
       "15    3.164221  3.502096  3.886642  2.358353  3.742648  3.733895  2.505191   \n",
       "16    3.006902  2.924975  2.117867  2.779568  2.481834  1.812072  3.007122   \n",
       "17    2.507914  3.341891  2.683169  2.300038  3.562616  2.949834  1.706754   \n",
       "18    3.294214  3.238111  2.600474  4.289855  2.391573  2.059570  3.632777   \n",
       "19    3.468367  1.942937  3.467535  2.315975  2.575120  2.936734  2.616287   \n",
       "20    2.131359  3.297008  2.393641  2.854704  3.558171  2.888006  2.835794   \n",
       "\n",
       "             H         I         J         K         L         M         N  \\\n",
       "User                                                                         \n",
       "1     2.673367  2.317369  3.521447  3.044063  2.934637  4.040158  2.998709   \n",
       "2     2.580919  3.000151  3.632670  3.096949  2.527106  2.173434  2.532439   \n",
       "3     1.978337  3.130212  1.539215  3.272523  2.279263  2.603694  2.982542   \n",
       "4     2.793358  3.117819  2.315239  2.864993  3.636110  1.807446  2.924626   \n",
       "5     3.083817  3.723310  2.617868  3.964756  3.011088  3.098395  3.409873   \n",
       "6     2.218895  2.993365  4.031558  3.873461  2.121943  4.000037  2.838593   \n",
       "7     3.567647  2.323389  2.179307  1.930943  2.658001  2.259856  2.175000   \n",
       "8     3.030631  2.282562  2.746412  2.828054  2.144753  2.837773  2.326532   \n",
       "9     3.731547  3.544526  2.412741  2.995182  3.572298  2.399526  3.780511   \n",
       "10    3.436299  4.137508  3.085499  3.176070  3.475341  2.360364  3.587308   \n",
       "11    3.105597  3.410328  2.394786  2.702986  3.240715  2.435015  3.194068   \n",
       "12    2.087771  2.568666  4.028625  2.512130  2.773786  3.304721  2.033686   \n",
       "13    4.226938  3.333401  3.939242  2.680818  3.029840  4.473829  3.975737   \n",
       "14    1.992951  2.431743  3.761554  2.868402  1.980609  3.012868  1.944916   \n",
       "15    2.661823  2.576640  4.332797  3.095291  2.249090  3.756429  2.332030   \n",
       "16    2.486361  3.197955  1.341647  3.016596  3.139452  3.066937  3.228790   \n",
       "17    2.656664  2.031370  3.960236  2.090184  2.912253  2.940406  2.399452   \n",
       "18    2.878757  3.496710  2.479210  3.745025  3.691556  2.841375  3.130646   \n",
       "19    2.204342  2.712550  3.073534  3.288228  1.735952  2.080328  1.840467   \n",
       "20    3.585306  2.282475  2.064412  2.515358  3.533807  3.549958  2.647616   \n",
       "\n",
       "             O         P         Q         R         S         T  \n",
       "User                                                              \n",
       "1     2.573670  4.267142  1.761662  4.332319  3.593288  2.319217  \n",
       "2     2.473829  3.200056  3.763495  1.881988  2.689880  1.752868  \n",
       "3     2.186740  1.321762  2.126969  2.364964  1.988240  3.365319  \n",
       "4     3.392369  2.389743  3.752023  1.487925  2.007544  3.683318  \n",
       "5     3.955804  2.317867  3.497197  3.571934  3.677112  3.248757  \n",
       "6     1.655562  3.619244  2.748993  3.299288  2.454697  2.453458  \n",
       "7     2.778515  1.777193  2.325736  2.792869  3.540116  2.824637  \n",
       "8     3.466282  3.619402  1.919753  3.713291  4.156083  2.860030  \n",
       "9     4.530988  1.999423  3.485569  2.459768  3.110284  4.022902  \n",
       "10    3.287793  2.135742  4.102399  1.827758  2.260609  3.852409  \n",
       "11    2.664088  1.636937  3.299081  2.259429  2.050339  3.394597  \n",
       "12    2.196265  3.994017  1.667015  2.966209  3.031615  1.985445  \n",
       "13    2.835887  3.519773  3.207538  4.253426  4.318568  3.229943  \n",
       "14    2.147991  3.834179  1.541710  2.795476  3.171459  2.230990  \n",
       "15    1.929799  4.202546  3.163350  3.466870  3.939457  2.038742  \n",
       "16    3.727807  2.755635  2.270164  3.699232  2.519407  3.912726  \n",
       "17    2.610773  3.999623  2.714805  3.082639  3.289360  1.601918  \n",
       "18    3.487406  2.861135  3.137600  2.415914  2.365042  3.712216  \n",
       "19    2.198956  1.675534  2.561586  2.003668  2.839879  2.348955  \n",
       "20    3.894996  3.681273  2.450744  4.433131  3.504492  3.270193  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colaborative_filtering(df,k_op)# the full matrix we predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86429700227436157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_df(df,colaborative_filtering(df,k_op)) #the rmse of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Netflix Dataset\n",
    "\n",
    "For the Netflix dataset, we began by researching the ways that previous teams had achieved accurate predictions. We found that the optimal solution was not create one model, but to train several models and then blend them together.\n",
    "\n",
    "We first took the training set and split it (75% training, 25% validation). We then chose our models. We found the Surprise package to be extremely useful, as it is a dedicated user recommendation system. From the Surprise package, we trained a selection of 7 models, including SVD, and KNN. We trained all 7 models on the training set, and then used them on to predict the ratings in the validation set. Our initial predictions ranged in RMSE, from 0.844 (SVDpp), to 0.9 (NMF). During this process we also made predictions for the 2000 observations in the test set, for each model.\n",
    "\n",
    "\n",
    "With these validation and test sets predictions, we began blending our models. To do this, we created a new dataset out of the predictions on our validation set, with each model as its features, and we split this into two subsets. We then trained a linear regression on one of these subsets, with the model predictions as our X variables, and the real ratings as our Y variable. We then used this linear regression to predict the ratings of the other validation subset. This gave us an estimated RMSE of 0.84. \n",
    "\n",
    "At this stage, we tried several other ways to blend the models. Despite trying more advanced models, such as a simple Neural network, a Random Forest and Elastic Net, Linear Regression was still either the most accurate, or of similar accuracy but with far lower computational complexity. \n",
    "\n",
    "Finally, we re-trained the linear regression on the entire validation set predictions, and then used this model to predict the ratings in the hold out test data set. Please see Netflix_Predictions.txt, for our final predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun  8 11:49:50 2018\n",
    "\n",
    "@author: Mark\n",
    "\"\"\"\n",
    "#Load core packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "#Load Surprise helpers\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "#Get all Models\n",
    "from surprise.prediction_algorithms import SVDpp\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans\n",
    "from surprise.prediction_algorithms import KNNWithZScore\n",
    "from surprise.prediction_algorithms import NMF\n",
    "from surprise.prediction_algorithms import KNNBaseline\n",
    "from surprise.prediction_algorithms import SlopeOne\n",
    "from surprise.prediction_algorithms import CoClustering\n",
    "\n",
    "\n",
    "#For final calculation\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "\n",
    "#Load in datasets\n",
    "path = r'C:/Users/Mark/Desktop/Marketing Analytics/Homework/Homework 3/netflix_HW3/Netflix_HW3_training.txt'\n",
    "df = pd.read_table(path, delimiter=\",\",\n",
    "                               names=['itemID', 'userID', 'ratings'],\n",
    "                               dtype={'itemID':np.uint32, 'userID':np.uint32, \n",
    "                                      'ratings':np.float})\n",
    "    \n",
    "\n",
    "order = ['userID', 'itemID', 'ratings']\n",
    "df = df[order]\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "del df\n",
    "\n",
    "#Split train test\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "path2 = r'C:/Users/Mark/Desktop/Marketing Analytics/Homework/Homework 3/netflix_HW3/Netflix_HW3_test.txt'\n",
    "df2 = pd.read_table(path2, delimiter=\",\",\n",
    "                               names=['itemID', 'userID'],\n",
    "                                dtype={'itemID':np.uint32, 'userID':np.uint32})\n",
    "    \n",
    "\n",
    "order2 = ['userID', 'itemID']\n",
    "df2 = df2[order2]\n",
    "\n",
    "\n",
    "#Dicts for storing results\n",
    "accuracies=dict() #Accuracy of each model\n",
    "predictions_dict=dict() #Get all predictions for each model\n",
    "final_predictions_dict=dict() #Get all predictions for each model\n",
    "\n",
    "\n",
    "#Create List of Models\n",
    "labels =['SVD','SVDpp','CoClustering','KNNBaseline','KNNWithMeans','NMF','SlopeOne']\n",
    "algos=dict() #Match each algo name to its model\n",
    "#set up algos\n",
    "algos[labels[0]]=  SVD(n_factors =30,n_epochs= 10, lr_all= 0.007, reg_all= 0.01,verbose=True)\n",
    "algos[labels[1]]=  SVDpp(verbose=True)\n",
    "algos[labels[2]]=  CoClustering(verbose=True)\n",
    "algos[labels[3]]=  KNNBaseline(verbose=True)\n",
    "algos[labels[4]]=  KNNWithMeans(verbose=True)\n",
    "algos[labels[5]]=  NMF(verbose=True)\n",
    "algos[labels[6]]=  SlopeOne()\n",
    "\n",
    "#Train all models\n",
    "for label,algo1 in algos.items():\n",
    "    #Fit algos\\\n",
    "    print(\"Start\",label)\n",
    "    algo=algo1\n",
    "    algo.fit(trainset)\n",
    "    print(\"Training done!\",label)\n",
    "    \n",
    "    prediction=algo.test(testset)\n",
    "    \n",
    "    \n",
    "    #Save accuracy\n",
    "    accuracies[label]=accuracy.rmse(prediction)\n",
    "    \n",
    "    #Save predictions for validation set\n",
    "    preds=[]\n",
    "    for a,b in enumerate(prediction):\n",
    "        preds.append(b[3])\n",
    "    predictions_dict[label]=preds\n",
    "    preds=[]\n",
    "    print(\"Validation Predicted\",label)\n",
    "    \n",
    "    \n",
    "    #Save predictions for test set\n",
    "    final_prediction=[]\n",
    "    for index, row in df2.iterrows():\n",
    "        final_prediction.append(algo.predict(row['userID'],row['itemID']).est)\n",
    "    final_predictions_dict[label]=final_prediction\n",
    "    final_prediction=[]\n",
    "    print(\"Test Predicted\",label)\n",
    "    \n",
    "\n",
    "#validation set real values\n",
    "real=[]\n",
    "for i in range(0,len(testset)): \n",
    "    real.append(testset[i][2])\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "#Dataframe of all predictions\n",
    "predictions=pd.DataFrame(predictions_dict)\n",
    "predictions['Real']=real  \n",
    "split_index = int(len(predictions)*0.75)\n",
    "\n",
    "predTrain = predictions.iloc[0:split_index]\n",
    "predTest = predictions.iloc[split_index:len(predictions)]\n",
    "\n",
    "\n",
    "#Set up blending on validation set\n",
    "\n",
    "\n",
    "# Find real labels\n",
    "y_test = np.array(predTrain['Real'])\n",
    "y_pred = np.array(predTest['Real'])\n",
    "\n",
    "\n",
    "X_test = predTrain.drop(['Real'], axis=1)\n",
    "X_test=np.array(X_test)\n",
    "\n",
    "X_pred = predTest.drop(['Real'], axis=1)\n",
    "X_pred=np.array(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find blending weights\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_test, y_test)\n",
    "\n",
    " # Create dictionary of weights\n",
    "weights = dict(zip(labels, linreg.coef_))\n",
    "\n",
    "# Predict final ratings\n",
    "final_predictions = np.clip(linreg.predict(X_pred), 1, 5)\n",
    "\n",
    "print('Blending Weights: ')\n",
    "print(weights, end='\\n\\n')\n",
    "\n",
    "print('RMSE on ProbeTrain: %f' % rmse(y_test, linreg.predict(X_test)))\n",
    "print('RMSE on ProbeTest: %f' % rmse(y_pred, final_predictions))\n",
    "predTest['Blended']=final_predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Predict on testset\n",
    "# Find real labels\n",
    "y_test2 = np.array(predictions['Real'])\n",
    "\n",
    "X_test2 = predictions.drop(['Real'], axis=1)\n",
    "X_test2=np.array(X_test2)\n",
    "\n",
    "X_pred2=pd.DataFrame(final_predictions_dict)\n",
    "X_pred2=np.array(X_pred2)\n",
    "\n",
    "\n",
    "\n",
    "# Find blending weights\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_test2, y_test2)\n",
    "\n",
    "# Create dictionary of weights\n",
    "weights = dict(zip(labels, linreg.coef_))\n",
    "\n",
    "# Predict final ratings\n",
    "final_predictions2 = np.clip(linreg.predict(X_pred2), 1, 5)\n",
    "\n",
    "print('Blending Weights: ')\n",
    "print(weights, end='\\n\\n')\n",
    "\n",
    "print('RMSE on ProbeTrain: %f' % rmse(y_test2, linreg.predict(X_test2)))\n",
    "df2['Rating']=final_predictions2\n",
    "\n",
    "#Export results\n",
    "export_order = ['itemID', 'userID', 'Rating']\n",
    "df2 = df2[export_order]\n",
    "df2.to_csv('Predictions.txt', sep=',', index=False,header=False)\n",
    "\n",
    "\n",
    "\n",
    "#Neural Networks regression using Keras package\n",
    "'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(7, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=1)\n",
    "\n",
    "estimator.fit(X_test, y_test)\n",
    "keras_prediction = estimator.predict(X_pred)\n",
    "rmse(y_pred, keras_prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find real labels\n",
    "y_test2 = np.array(predictions['Real'])\n",
    "\n",
    "X_test2 = predictions.drop(['Real'], axis=1)\n",
    "X_test2=np.array(X_test2)\n",
    "\n",
    "X_pred2=pd.DataFrame(final_predictions_dict)\n",
    "X_pred2=np.array(X_pred2)\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=1)\n",
    "\n",
    "estimator.fit(X_test2, y_test2)\n",
    "keras_prediction = estimator.predict(X_pred2,batch_size=5)\n",
    "rmse(y_test2, X_test2)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
